{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Maintenance Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import string\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import awswrangler as wr\n",
    "\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = sagemaker.Session().boto_region_name\n",
    "print(\"Using AWS Region: {}\".format(region))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.setup_default_session(region_name = region)\n",
    "boto_session = boto3.Session(region_name = region)\n",
    "\n",
    "s3_client = boto3.client(\"s3\", region_name = region)\n",
    "\n",
    "sagemaker_boto_client = boto_session.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.session.Session(\n",
    "    boto_session = boto_session, sagemaker_client = sagemaker_boto_client\n",
    ")\n",
    "account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "pipeline_session = PipelineSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"BUCKET-NAME\"\n",
    "prefix = \"mlops/predictive-maintenance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload raw data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "s3_client.upload_file(\n",
    "    Filename=\"datasets/PdM_telemetry.csv\", Bucket=bucket, Key=f\"{prefix}/data/raw/PdM_telemetry.csv\"\n",
    ")\n",
    "s3_client.upload_file(\n",
    "    Filename=\"datasets/PdM_errors.csv\", Bucket=bucket, Key=f\"{prefix}/data/raw/PdM_errors.csv\"\n",
    ")\n",
    "s3_client.upload_file(\n",
    "    Filename=\"datasets/PdM_maint.csv\", Bucket=bucket, Key=f\"{prefix}/data/raw/PdM_maint.csv\"\n",
    ")\n",
    "s3_client.upload_file(\n",
    "    Filename=\"datasets/PdM_failures.csv\", Bucket=bucket, Key=f\"{prefix}/data/raw/PdM_failures.csv\"\n",
    ")\n",
    "s3_client.upload_file(\n",
    "    Filename=\"datasets/PdM_machines.csv\", Bucket=bucket, Key=f\"{prefix}/data/raw/PdM_machines.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install s3fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "telemetry_data_uri = f\"s3://{bucket}/{prefix}/data/raw/PdM_telemetry.csv\"\n",
    "errors_data_uri = f\"s3://{bucket}/{prefix}/data/raw/PdM_errors.csv\"\n",
    "maint_data_uri = f\"s3://{bucket}/{prefix}/data/raw/PdM_maint.csv\"\n",
    "failures_data_uri = f\"s3://{bucket}/{prefix}/data/raw/PdM_failures.csv\"\n",
    "machines_data_uri = f\"s3://{bucket}/{prefix}/data/raw/PdM_machines.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>machineID</th>\n",
       "      <th>volt</th>\n",
       "      <th>rotate</th>\n",
       "      <th>pressure</th>\n",
       "      <th>vibration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01 06:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>176.217853</td>\n",
       "      <td>418.504078</td>\n",
       "      <td>113.077935</td>\n",
       "      <td>45.087686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-01 07:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>162.879223</td>\n",
       "      <td>402.747490</td>\n",
       "      <td>95.460525</td>\n",
       "      <td>43.413973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-01 08:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>170.989902</td>\n",
       "      <td>527.349825</td>\n",
       "      <td>75.237905</td>\n",
       "      <td>34.178847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-01 09:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>162.462833</td>\n",
       "      <td>346.149335</td>\n",
       "      <td>109.248561</td>\n",
       "      <td>41.122144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-01 10:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>157.610021</td>\n",
       "      <td>435.376873</td>\n",
       "      <td>111.886648</td>\n",
       "      <td>25.990511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime  machineID        volt      rotate    pressure  \\\n",
       "0  2015-01-01 06:00:00          1  176.217853  418.504078  113.077935   \n",
       "1  2015-01-01 07:00:00          1  162.879223  402.747490   95.460525   \n",
       "2  2015-01-01 08:00:00          1  170.989902  527.349825   75.237905   \n",
       "3  2015-01-01 09:00:00          1  162.462833  346.149335  109.248561   \n",
       "4  2015-01-01 10:00:00          1  157.610021  435.376873  111.886648   \n",
       "\n",
       "   vibration  \n",
       "0  45.087686  \n",
       "1  43.413973  \n",
       "2  34.178847  \n",
       "3  41.122144  \n",
       "4  25.990511  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = s3_client.get_object(Bucket = bucket, Key = f\"{prefix}/data/raw/PdM_telemetry.csv\")\n",
    "telemetry = pd.read_csv(response.get(\"Body\"))\n",
    "telemetry.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "telemetry = pd.read_csv(telemetry_data_uri)\n",
    "# errors = wr.s3.read_csv(errors_data_uri)\n",
    "# maint = wr.s3.read_csv(maint_data_uri)\n",
    "# failures = wr.s3.read_csv(failures_data_uri)\n",
    "# machines = wr.s3.read_csv(machines_data_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Parameters to Parametrize Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    "    ParameterFloat,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_uri = f\"s3://{bucket}/{prefix}/data/final_data.csv\"\n",
    "batch_data_uri = f\"s3://{bucket}/{prefix}/data/train-test/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_instance_count = ParameterInteger(name = \"ProcessingInstanceCount\", default_value = 1)\n",
    "instance_type = ParameterString(name = \"TrainingInstanceType\", default_value = \"ml.m5.xlarge\")\n",
    "\n",
    "model_approval_status = ParameterString(\n",
    "    name = \"ModelApprovalStatus\", default_value = \"PendingManualApproval\"\n",
    ")\n",
    "\n",
    "input_data = ParameterString(\n",
    "    name = \"InputData\",\n",
    "    default_value = input_data_uri,\n",
    ")\n",
    "\n",
    "telemetry_data = ParameterString(\n",
    "    name = \"TelemetryData\",\n",
    "    default_value = f\"s3://{bucket}/{prefix}/data/raw/PdM_telemetry.csv\",\n",
    ")\n",
    "\n",
    "errors_data = ParameterString(\n",
    "    name = \"ErrorsData\",\n",
    "    default_value = f\"s3://{bucket}/{prefix}/data/raw/PdM_errors.csv\",\n",
    ")\n",
    "\n",
    "maint_data = ParameterString(\n",
    "    name = \"MaintenanceData\",\n",
    "    default_value = f\"s3://{bucket}/{prefix}/data/raw/PdM_maint.csv\",\n",
    ")\n",
    "\n",
    "failures_data = ParameterString(\n",
    "    name = \"FailuresData\",\n",
    "    default_value = f\"s3://{bucket}/{prefix}/data/raw/PdM_failures.csv\",\n",
    ")\n",
    "\n",
    "machines_data = ParameterString(\n",
    "    name = \"MachinesData\",\n",
    "    default_value = f\"s3://{bucket}/{prefix}/data/raw/PdM_machines.csv\",\n",
    ")\n",
    "\n",
    "batch_data = ParameterString(\n",
    "    name = \"BatchData\",\n",
    "    default_value = batch_data_uri,\n",
    ")\n",
    "\n",
    "mse_threshold = ParameterFloat(name = \"MseThreshold\", default_value = 6.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Processing Step for Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting processing_scripts/preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile processing_scripts/preprocessing.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from io import StringIO\n",
    "import awswrangler as wr\n",
    "\n",
    "base_dir = \"/opt/ml/processing\"\n",
    "bucket = \"BUCKET-NAME\"\n",
    "prefix = \"mlops/predictive-maintenance\"\n",
    "\n",
    "def upload_file_s3(df, name):\n",
    "    boto3.setup_default_session(region_name = \"us-east-1\")\n",
    "    s3_client = boto3.client(\"s3\", region_name = \"us-east-1\")\n",
    "    with StringIO() as csv_buffer:\n",
    "        df.to_csv(csv_buffer, index = False)\n",
    "\n",
    "        response = s3_client.put_object(\n",
    "            Bucket = bucket, Key = f\"{prefix}/data/preprocessed/{name}.csv\", Body = csv_buffer.getvalue()\n",
    "        )\n",
    "        status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
    "\n",
    "        if status == 200:\n",
    "            print(f\"Successful S3 put_object response. Status - {status}\")\n",
    "        else:\n",
    "            print(f\"Unsuccessful S3 put_object response. Status - {status}\")\n",
    "\n",
    "# Convert to datetime datatype\n",
    "def datetime_datatype(df):\n",
    "    print(\"Converting to type datetime\")\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Convert to category datatype\n",
    "def category_datatype(df, column_name):\n",
    "    print(\"Converting to type category\")\n",
    "    df[column_name] = df[column_name].astype('category')\n",
    "    return df\n",
    "\n",
    "\n",
    "# Lag Features from Telemetry\n",
    "def telemetry_features(df):\n",
    "    df = datetime_datatype(df)\n",
    "    # Calculate mean values for telemetry features -- 3 hours rolling window\n",
    "    print(\"Calculate mean values for telemetry features -- 3 hours rolling window\")\n",
    "    temp = []\n",
    "    fields = ['volt', 'rotate', 'pressure', 'vibration']\n",
    "    for col in fields:\n",
    "        temp.append(pd.pivot_table(df,\n",
    "                                   index = 'datetime',\n",
    "                                   columns = 'machineID',\n",
    "                                   values = col).resample('3H', closed = 'left', label = 'right').mean().unstack())\n",
    "    telemetry_mean_3h = pd.concat(temp, axis = 1)\n",
    "    telemetry_mean_3h.columns = [i + 'mean_3h' for i in fields]\n",
    "    telemetry_mean_3h.reset_index(inplace = True)\n",
    "\n",
    "    # repeat for standard deviation\n",
    "    print(\"Calculate standard deviation for telemetry features -- 3 hours rolling window\")\n",
    "    temp = []\n",
    "    for col in fields:\n",
    "        temp.append(pd.pivot_table(df,\n",
    "                                   index = 'datetime',\n",
    "                                   columns = 'machineID',\n",
    "                                   values = col).resample('3H', closed = 'left', label = 'right').std().unstack())\n",
    "    telemetry_sd_3h = pd.concat(temp, axis = 1)\n",
    "    telemetry_sd_3h.columns = [i + 'sd_3h' for i in fields]\n",
    "    telemetry_sd_3h.reset_index(inplace = True)\n",
    "    \n",
    "    # Calculate mean values for telemetry features -- 24 hours rolling window\n",
    "    print(\"Calculate mean values for telemetry features -- 24 hours rolling window\")\n",
    "    temp = []\n",
    "    fields = ['volt', 'rotate', 'pressure', 'vibration']\n",
    "    for col in fields:\n",
    "        temp.append(pd.pivot_table(df,\n",
    "                                   index = 'datetime',\n",
    "                                   columns = 'machineID',\n",
    "                                   values = col)\n",
    "                    .resample('3H', closed = 'left', label = 'right')\n",
    "                    .first()\n",
    "                    .unstack()\n",
    "                    .rolling(window = 24, center = False).mean())\n",
    "    telemetry_mean_24h = pd.concat(temp, axis = 1)\n",
    "    telemetry_mean_24h.columns = [i + 'mean_24h' for i in fields]\n",
    "    telemetry_mean_24h.reset_index(inplace = True)\n",
    "    telemetry_mean_24h = telemetry_mean_24h.loc[-telemetry_mean_24h['voltmean_24h'].isnull()]\n",
    "\n",
    "    # repeat for standard deviation\n",
    "    print(\"Calculate standard deviation for telemetry features -- 24 hours rolling window\")\n",
    "    temp = []\n",
    "    fields = ['volt', 'rotate', 'pressure', 'vibration']\n",
    "    for col in fields:\n",
    "        temp.append(pd.pivot_table(df,\n",
    "                                   index = 'datetime',\n",
    "                                   columns = 'machineID',\n",
    "                                   values = col)\n",
    "                    .resample('3H', closed='left', label='right')\n",
    "                    .first()\n",
    "                    .unstack()\n",
    "                    .rolling(window = 24, center = False).std())\n",
    "    telemetry_sd_24h = pd.concat(temp, axis = 1)\n",
    "    telemetry_sd_24h.columns = [i + 'sd_24h' for i in fields]\n",
    "    telemetry_sd_24h = telemetry_sd_24h.loc[-telemetry_sd_24h['voltsd_24h'].isnull()]\n",
    "    telemetry_sd_24h.reset_index(inplace = True)\n",
    "    \n",
    "    telemetry_feat = pd.concat([telemetry_mean_3h,\n",
    "                            telemetry_sd_3h.iloc[:, 2:6],\n",
    "                            telemetry_mean_24h.iloc[:, 2:6],\n",
    "                            telemetry_sd_24h.iloc[:, 2:6]], axis = 1).dropna()\n",
    "\n",
    "    upload_file_s3(telemetry_feat, \"telemetry\")\n",
    "    \n",
    "    return telemetry_feat\n",
    "\n",
    "\n",
    "# Lag Features for Errors\n",
    "def errors_lag_features(df):\n",
    "    df = datetime_datatype(df)\n",
    "    df = category_datatype(df, 'errorID')\n",
    "    print(\"Lag features for errors\")\n",
    "    error_count = pd.get_dummies(df.set_index('datetime')).reset_index()\n",
    "    error_count.columns = ['datetime', 'machineID', 'error1', 'error2', 'error3', 'error4', 'error5']\n",
    "    error_count = error_count.groupby(['machineID', 'datetime']).sum().reset_index()\n",
    "    error_count = telemetry[['datetime', 'machineID']].merge(error_count, on = ['machineID', 'datetime'], how = 'left').fillna(0.0)\n",
    "    temp = []\n",
    "    fields = ['error%d' % i for i in range(1, 6)]\n",
    "    for col in fields:\n",
    "        temp.append(pd.pivot_table(error_count,\n",
    "                                   index = 'datetime',\n",
    "                                   columns = 'machineID',\n",
    "                                   values = col)\n",
    "                    .resample('3H', closed='left', label='right')\n",
    "                    .first()\n",
    "                    .unstack()\n",
    "                    .rolling(window = 24, center = False).sum())\n",
    "    error_count = pd.concat(temp, axis = 1)\n",
    "    error_count.columns = [i + 'count' for i in fields]\n",
    "    error_count.reset_index(inplace = True)\n",
    "    error_count = error_count.dropna()\n",
    "    \n",
    "    upload_file_s3(error_count, \"errors\")\n",
    "    \n",
    "    return error_count\n",
    "\n",
    "\n",
    "# Maintenance Features\n",
    "def maintenance_features(df):\n",
    "    df = datetime_datatype(df)\n",
    "    df = category_datatype(df, 'comp')\n",
    "    print(\"Maintenance Features -- Days since last replacement\")\n",
    "    comp_rep = pd.get_dummies(df.set_index('datetime')).reset_index()\n",
    "    comp_rep.columns = ['datetime', 'machineID', 'comp1', 'comp2', 'comp3', 'comp4']\n",
    "\n",
    "    # combine repairs for a given machine in a given hour\n",
    "    comp_rep = comp_rep.groupby(['machineID', 'datetime']).sum().reset_index()\n",
    "\n",
    "    # add timepoints where no components were replaced\n",
    "    comp_rep = telemetry[['datetime', 'machineID']].merge(comp_rep,\n",
    "                                                          on=['datetime', 'machineID'],\n",
    "                                                          how='outer').fillna(0).sort_values(by=['machineID', 'datetime'])\n",
    "    components = ['comp1', 'comp2', 'comp3', 'comp4']\n",
    "    for comp in components:\n",
    "        comp_rep.loc[comp_rep[comp] < 1, comp] = None\n",
    "        comp_rep.loc[-comp_rep[comp].isnull(),\n",
    "                     comp] = comp_rep.loc[-comp_rep[comp].isnull(), 'datetime']\n",
    "        comp_rep[comp] = comp_rep[comp].fillna(method = 'ffill')\n",
    "\n",
    "    comp_rep = comp_rep.loc[comp_rep['datetime'] > pd.to_datetime('2015-01-01')]\n",
    "    for comp in components:\n",
    "        comp_rep[comp] = (comp_rep[\"datetime\"] - pd.to_datetime(comp_rep[comp])) / np.timedelta64(1, \"D\")\n",
    "        \n",
    "    upload_file_s3(comp_rep, \"maint\")\n",
    "    \n",
    "    return comp_rep\n",
    "\n",
    "\n",
    "# Failures Features\n",
    "def failure_features(df):\n",
    "    print(\"Failure features\")\n",
    "    df = datetime_datatype(df)\n",
    "    df = category_datatype(df, 'failure')\n",
    "    upload_file_s3(df, \"failures\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Final Features\n",
    "def final_features(telemetry_df, errors_df, maint_df, machines_df):\n",
    "    upload_file_s3(machines_df, \"machines\")\n",
    "    print(\"Final features\")\n",
    "    final_feat = telemetry_df.merge(errors_df, on = ['datetime', 'machineID'], how = 'left')\n",
    "    final_feat = final_feat.merge(maint_df, on = ['datetime', 'machineID'], how = 'left')\n",
    "    final_feat = final_feat.merge(machines_df, on = ['machineID'], how = 'left')\n",
    "    return final_feat\n",
    "\n",
    "\n",
    "# Label Construction\n",
    "def label_construct(tele_df, error_df, maint_df, machine_df, failure_df):\n",
    "    print(\"----- Final Features -----\")\n",
    "    final_feat = final_features(tele_df, error_df, maint_df, machine_df)\n",
    "    \n",
    "    print(\"----- Label Construction -----\")\n",
    "    labeled_features = pd.DataFrame()\n",
    "    labeled_features = final_feat.merge(\n",
    "        failure_df, on = ['datetime', 'machineID'], how = 'left')\n",
    "    labeled_features['failure'] = labeled_features['failure'].astype(str)\n",
    "    labeled_features['failure'] = labeled_features['failure'].fillna(method = 'bfill', limit = 7)\n",
    "    labeled_features['failure'] = labeled_features['failure'].replace('nan', 'none')\n",
    "    print(\"----- Preprocessing completed -----\")\n",
    "    \n",
    "    upload_file_s3(labeled_features, \"final_data\")\n",
    "#     pd.DataFrame(labeled_features).to_csv(f\"{base_dir}/preprocessed/final_data.csv\", index = False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    telemetry_data_uri = f\"s3://{bucket}/{prefix}/data/raw/PdM_telemetry.csv\"\n",
    "    errors_data_uri = f\"s3://{bucket}/{prefix}/data/raw/PdM_errors.csv\"\n",
    "    maint_data_uri = f\"s3://{bucket}/{prefix}/data/raw/PdM_maint.csv\"\n",
    "    failures_data_uri = f\"s3://{bucket}/{prefix}/data/raw/PdM_failures.csv\"\n",
    "    machines_data_uri = f\"s3://{bucket}/{prefix}/data/raw/PdM_machines.csv\"\n",
    "    \n",
    "    telemetry = wr.s3.read_csv(telemetry_data_uri)\n",
    "    errors = wr.s3.read_csv(errors_data_uri)\n",
    "    maint = wr.s3.read_csv(maint_data_uri)\n",
    "    failures = wr.s3.read_csv(failures_data_uri)\n",
    "    machines = wr.s3.read_csv(machines_data_uri)\n",
    "    \n",
    "    telemetry_df = telemetry_features(telemetry)\n",
    "    errors_df = errors_lag_features(errors)\n",
    "    maint_df = maintenance_features(maint)\n",
    "    failures_df = failure_features(failures)\n",
    "    machines_df = category_datatype(machines, 'model')\n",
    "    \n",
    "    label_construct(telemetry_df, errors_df, maint_df, machines_df, failures_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Store Creation Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting processing_scripts/featurestore.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile processing_scripts/featurestore.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "import awswrangler as wr\n",
    "\n",
    "base_dir = \"/opt/ml/processing\"\n",
    "bucket = \"BUCKET-NAME\"\n",
    "prefix = \"mlops/predictive-maintenance\"\n",
    "\n",
    "boto_session = boto3.Session(region_name = \"us-east-1\")\n",
    "sagemaker_boto_client = boto_session.client(\"sagemaker\")\n",
    "featurestore_runtime = boto_session.client(\n",
    "    service_name = \"sagemaker-featurestore-runtime\", region_name = \"us-east-1\"\n",
    ")\n",
    "try:\n",
    "    sagemaker_role = sagemaker.get_execution_role()\n",
    "    print(f\"Sagemaker Role for Feature Store file: {sagemaker_role}\")\n",
    "except ValueError:\n",
    "    sagemaker_role = 'SAGEMAKER-ROLE'\n",
    "    \n",
    "feature_store_session = sagemaker.Session(\n",
    "    boto_session = boto_session,\n",
    "    sagemaker_client = sagemaker_boto_client,\n",
    "    sagemaker_featurestore_runtime_client = featurestore_runtime,\n",
    ")\n",
    "\n",
    "# ------------------------------------ Read Data\n",
    "telemetry_data_uri = f\"s3://{bucket}/{prefix}/data/preprocessed/telemetry.csv\"\n",
    "errors_data_uri = f\"s3://{bucket}/{prefix}/data/preprocessed/errors.csv\"\n",
    "maint_data_uri = f\"s3://{bucket}/{prefix}/data/preprocessed/maint.csv\"\n",
    "failures_data_uri = f\"s3://{bucket}/{prefix}/data/preprocessed/failures.csv\"\n",
    "machines_data_uri = f\"s3://{bucket}/{prefix}/data/preprocessed/machines.csv\"\n",
    "\n",
    "telemetry = wr.s3.read_csv(telemetry_data_uri)\n",
    "errors = wr.s3.read_csv(errors_data_uri)\n",
    "maint = wr.s3.read_csv(maint_data_uri)\n",
    "failures = wr.s3.read_csv(failures_data_uri)\n",
    "machines = wr.s3.read_csv(machines_data_uri)\n",
    "\n",
    "# ------------------------------------ Add Timestamp\n",
    "telemetry['event_time'] = pd.to_datetime(\"now\").timestamp()\n",
    "errors['event_time'] = pd.to_datetime(\"now\").timestamp()\n",
    "maint['event_time'] = pd.to_datetime(\"now\").timestamp()\n",
    "failures['event_time'] = pd.to_datetime(\"now\").timestamp()\n",
    "machines['event_time'] = pd.to_datetime(\"now\").timestamp()\n",
    "\n",
    "# ------------------------------------ Create Feature Group\n",
    "telemetry_feature_group = FeatureGroup(name = 'telemetry_fg', sagemaker_session = feature_store_session)\n",
    "errors_feature_group = FeatureGroup(name = 'errors_fg', sagemaker_session = feature_store_session)\n",
    "maintenance_feature_group = FeatureGroup(name = 'maintenance_fg', sagemaker_session = feature_store_session)\n",
    "failures_feature_group = FeatureGroup(name = 'failures_fg', sagemaker_session = feature_store_session)\n",
    "machines_feature_group = FeatureGroup(name = 'machines_fg', sagemaker_session = feature_store_session)\n",
    "\n",
    "# ------------------------------------ Loading Definitions\n",
    "telemetry_feature_group.load_feature_definitions(data_frame = telemetry)\n",
    "errors_feature_group.load_feature_definitions(data_frame = errors)\n",
    "maintenance_feature_group.load_feature_definitions(data_frame = maint)\n",
    "failures_feature_group.load_feature_definitions(data_frame = failures)\n",
    "machines_feature_group.load_feature_definitions(data_frame = machines)\n",
    "\n",
    "record_identifier_feature_name = \"machineID\"\n",
    "event_time_feature_name = \"event_time\"\n",
    "\n",
    "# ------------------------------------ Telemetry Feature Store\n",
    "try:\n",
    "    telemetry_feature_group.create(\n",
    "        s3_uri = f\"s3://{bucket}/{prefix}/feature_store_data\",\n",
    "        record_identifier_name = record_identifier_feature_name,\n",
    "        event_time_feature_name = event_time_feature_name,\n",
    "        role_arn = sagemaker_role,\n",
    "        enable_online_store = True,\n",
    "    )\n",
    "    time.sleep(30)\n",
    "    print(f'Create \"telemetry\" feature group: SUCCESS')\n",
    "except Exception as e:\n",
    "    code = e.response.get(\"Error\").get(\"Code\")\n",
    "    if code == \"ResourceInUse\":\n",
    "        print(\"Using existing feature group\")\n",
    "    else:\n",
    "        raise (e)\n",
    "\n",
    "# ------------------------------------ Errors Feature Store      \n",
    "try:\n",
    "    errors_feature_group.create(\n",
    "        s3_uri = f\"s3://{bucket}/{prefix}/feature_store_data\",\n",
    "        record_identifier_name = record_identifier_feature_name,\n",
    "        event_time_feature_name = event_time_feature_name,\n",
    "        role_arn = sagemaker_role,\n",
    "        enable_online_store = True,\n",
    "    )\n",
    "    time.sleep(30)\n",
    "    print(f'Create \"errors\" feature group: SUCCESS')\n",
    "except Exception as e:\n",
    "    code = e.response.get(\"Error\").get(\"Code\")\n",
    "    if code == \"ResourceInUse\":\n",
    "        print(\"Using existing feature group\")\n",
    "    else:\n",
    "        raise (e)\n",
    "\n",
    "# ------------------------------------ Maintenance Feature Store\n",
    "try:\n",
    "    maintenance_feature_group.create(\n",
    "        s3_uri = f\"s3://{bucket}/{prefix}/feature_store_data\",\n",
    "        record_identifier_name = record_identifier_feature_name,\n",
    "        event_time_feature_name = event_time_feature_name,\n",
    "        role_arn = sagemaker_role,\n",
    "        enable_online_store = True,\n",
    "    )\n",
    "    time.sleep(30)\n",
    "    print(f'Create \"maintenance\" feature group: SUCCESS')\n",
    "except Exception as e:\n",
    "    code = e.response.get(\"Error\").get(\"Code\")\n",
    "    if code == \"ResourceInUse\":\n",
    "        print(\"Using existing feature group\")\n",
    "    else:\n",
    "        raise (e)\n",
    "\n",
    "# ------------------------------------ Failures Feature Store\n",
    "try:\n",
    "    failures_feature_group.create(\n",
    "        s3_uri = f\"s3://{bucket}/{prefix}/feature_store_data\",\n",
    "        record_identifier_name = record_identifier_feature_name,\n",
    "        event_time_feature_name = event_time_feature_name,\n",
    "        role_arn = sagemaker_role,\n",
    "        enable_online_store = True,\n",
    "    )\n",
    "    time.sleep(30)\n",
    "    print(f'Create \"failures\" feature group: SUCCESS')\n",
    "except Exception as e:\n",
    "    code = e.response.get(\"Error\").get(\"Code\")\n",
    "    if code == \"ResourceInUse\":\n",
    "        print(\"Using existing feature group\")\n",
    "    else:\n",
    "        raise (e)\n",
    "\n",
    "# ------------------------------------ Machines Feature Store\n",
    "try:\n",
    "    machines_feature_group.create(\n",
    "        s3_uri = f\"s3://{bucket}/{prefix}/feature_store_data\",\n",
    "        record_identifier_name = record_identifier_feature_name,\n",
    "        event_time_feature_name = event_time_feature_name,\n",
    "        role_arn = sagemaker_role,\n",
    "        enable_online_store = True,\n",
    "    )\n",
    "    time.sleep(30)\n",
    "    print(f'Create \"machines\" feature group: SUCCESS')\n",
    "except Exception as e:\n",
    "    code = e.response.get(\"Error\").get(\"Code\")\n",
    "    if code == \"ResourceInUse\":\n",
    "        print(\"Using existing feature group\")\n",
    "    else:\n",
    "        raise (e)\n",
    "\n",
    "# ------------------------------------ Ingesting Data\n",
    "while (\n",
    "    telemetry_feature_group.describe()['FeatureGroupStatus'] == 'Creating'):\n",
    "    print(\"Feature Group Creating\")\n",
    "    time.sleep(60)\n",
    "else:\n",
    "    print(\"Feature Group Created\")\n",
    "#     telemetry_feature_group.ingest(data_frame = telemetry, max_workers = 3, wait = True)\n",
    "#     errors_feature_group.ingest(data_frame = errors, max_workers = 3, wait = True)\n",
    "#     maintenance_feature_group.ingest(data_frame = maint, max_workers = 3, wait = True)\n",
    "#     failures_feature_group.ingest(data_frame = failures, max_workers = 3, wait = True)\n",
    "#     machines_feature_group.ingest(data_frame = machines, max_workers = 3, wait = True)\n",
    "    print(\"Feature Data Ingested\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting processing_scripts/train_test_split_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile processing_scripts/train_test_split_data.py\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from io import StringIO\n",
    "import awswrangler as wr\n",
    "\n",
    "base_dir = \"/opt/ml/processing\"\n",
    "bucket = \"BUCKET-NAME\"\n",
    "prefix = \"mlops/predictive-maintenance\"\n",
    "\n",
    "def upload_file_s3(df, name):\n",
    "    boto3.setup_default_session(region_name = \"us-east-1\")\n",
    "    s3_client = boto3.client(\"s3\", region_name = \"us-east-1\")\n",
    "    with StringIO() as csv_buffer:\n",
    "        df.to_csv(csv_buffer, index = False)\n",
    "\n",
    "        response = s3_client.put_object(\n",
    "            Bucket = bucket, Key = f\"{prefix}/data/train-test/{name}.csv\", Body = csv_buffer.getvalue()\n",
    "        )\n",
    "        status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
    "\n",
    "        if status == 200:\n",
    "            print(f\"Successful S3 put_object response. Status - {status}\")\n",
    "        else:\n",
    "            print(f\"Unsuccessful S3 put_object response. Status - {status}\")\n",
    "\n",
    "def train_test_split_script(labeled_features):\n",
    "    threshold_dates = [[pd.to_datetime('2015-07-31 01:00:00'), pd.to_datetime('2015-08-01 01:00:00')],\n",
    "                   [pd.to_datetime('2015-08-31 01:00:00'), pd.to_datetime('2015-09-01 01:00:00')],\n",
    "                   [pd.to_datetime('2015-09-30 01:00:00'), pd.to_datetime('2015-10-01 01:00:00')]]\n",
    "    \n",
    "    for last_train_date, first_test_date in threshold_dates:\n",
    "        # split out training and test data\n",
    "        print(labeled_features['datetime'][0])\n",
    "        train_y = labeled_features.loc[labeled_features['datetime'] < last_train_date, 'failure']\n",
    "        train_data = pd.get_dummies(labeled_features.loc[labeled_features['datetime'] < last_train_date].drop(['datetime',\n",
    "                                                                                                            'machineID',\n",
    "                                                                                                              'failure'], axis = 1))\n",
    "        test_y = labeled_features.loc[labeled_features['datetime'] > last_train_date, 'failure']\n",
    "        test_data = pd.get_dummies(labeled_features.loc[labeled_features['datetime'] > first_test_date].drop(['datetime',\n",
    "                                                                                                           'machineID',\n",
    "                                                                                                             'failure'], axis = 1))\n",
    "    \n",
    "    train_data['failure'] = train_y\n",
    "    test_data['failure'] = test_y\n",
    "    \n",
    "    upload_file_s3(train_data, \"train\")\n",
    "    upload_file_s3(test_data, \"test\")\n",
    "\n",
    "    pd.DataFrame(train_data).to_csv(f\"{base_dir}/train/train.csv\", index = False)\n",
    "    pd.DataFrame(test_data).to_csv(f\"{base_dir}/test/test.csv\", index = False)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    final_data_uri = f\"s3://{bucket}/{prefix}/data/preprocessed/final_data.csv\"\n",
    "    final_data = wr.s3.read_csv(final_data_uri)\n",
    "    final_data['datetime'] = pd.to_datetime(final_data['datetime'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    train_test_split_script(final_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an instance of a SKLearnProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import FrameworkProcessor\n",
    "from sagemaker.sklearn import SKLearn\n",
    "framework_version = \"1.2-1\"\n",
    "\n",
    "sklearn_processor = FrameworkProcessor(\n",
    "    estimator_cls = SKLearn,\n",
    "    framework_version = framework_version,\n",
    "    instance_type = \"ml.m5.xlarge\",\n",
    "    instance_count = processing_instance_count,\n",
    "    base_job_name = \"sklearn-pred-maint-process\",\n",
    "    role = sagemaker_role,\n",
    "    sagemaker_session = pipeline_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Processing Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sagemaker/workflow/pipeline_context.py:261: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "processor_args = sklearn_processor.run(\n",
    "#     inputs = [\n",
    "#         ProcessingInput(source = telemetry_data, destination = \"/opt/ml/processing/input/telemetry\"),\n",
    "#         ProcessingInput(source = errors_data, destination = \"/opt/ml/processing/input/errors\"),\n",
    "#         ProcessingInput(source = maint_data, destination = \"/opt/ml/processing/input/maint\"),\n",
    "#         ProcessingInput(source = failures_data, destination = \"/opt/ml/processing/input/failures\"),\n",
    "#         ProcessingInput(source = machines_data, destination = \"/opt/ml/processing/input/machines\"),\n",
    "#     ],\n",
    "#     outputs = [\n",
    "#         ProcessingOutput(output_name = \"final_data\", source = \"/opt/ml/processing/preprocessed\"),\n",
    "# #         ProcessingOutput(output_name = \"test\", source = \"/opt/ml/processing/test\"),\n",
    "#     ],\n",
    "    code = \"preprocessing.py\",\n",
    "    source_dir = \"processing_scripts\"\n",
    ")\n",
    "\n",
    "preprocess_step = ProcessingStep(name = \"PdM-Data-Read-And-PreProcessing\", step_args = processor_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_args = sklearn_processor.run(\n",
    "#     inputs = [\n",
    "#         ProcessingInput(source = input_data, destination = \"/opt/ml/processing/processing/preprocessed\"),\n",
    "#     ],\n",
    "    outputs = [\n",
    "        ProcessingOutput(output_name = \"train\", source = \"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(output_name = \"test\", source = \"/opt/ml/processing/test\"),\n",
    "    ],\n",
    "    code = \"train_test_split_data.py\",\n",
    "    source_dir = \"processing_scripts\"\n",
    ")\n",
    "\n",
    "train_test_split_step = ProcessingStep(name = \"PdM-Train-Test-Data-Split\", step_args = train_test_args, depends_on = [preprocess_step.name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_data = sklearn_processor.run(\n",
    "    code = \"featurestore.py\",\n",
    "    source_dir = \"processing_scripts\"\n",
    ")\n",
    "feature_store_step = ProcessingStep(name = \"PdM-FeatureStore-Creation\", step_args = fs_data, depends_on = [preprocess_step.name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Training Step to Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.inputs import TrainingInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_estimator = SKLearn(\n",
    "    entry_point = \"scripts/rf_script.py\",\n",
    "    role = sagemaker_role,\n",
    "    instance_count = 1,\n",
    "    instance_type = \"ml.m5.xlarge\",\n",
    "    framework_version = \"1.2-1\",\n",
    "    base_job_name = \"rf-scikit\",\n",
    "    hyperparameters = {\n",
    "        \"n-estimators\": 100,\n",
    "        \"min-samples-leaf\": 3,\n",
    "    },\n",
    "    sagemaker_session = pipeline_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = sklearn_estimator.fit(\n",
    "    inputs = {\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data = train_test_split_step.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "#             s3_data = f\"s3://{bucket}/{prefix}/data/train-test/train.csv\",\n",
    "            content_type = \"text/csv\"\n",
    "        ),\n",
    "        \"test\": TrainingInput(\n",
    "            s3_data = train_test_split_step.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,\n",
    "#             s3_data = f\"s3://{bucket}/{prefix}/data/train-test/test.csv\",\n",
    "            content_type = \"text/csv\",\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "training_step = TrainingStep(\n",
    "    name = \"PdM-ModelTraininig\",\n",
    "    step_args = train_args,\n",
    "    depends_on = [train_test_split_step.name]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Create Model Step to Create a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "\n",
    "model = SKLearnModel(\n",
    "    entry_point = \"scripts/rf_script.py\",\n",
    "    model_data = preprocess_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session = pipeline_session,\n",
    "    framework_version = \"1.2-1\",\n",
    "    role = sagemaker_role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sagemaker.inputs import CreateModelInput\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "\n",
    "step_create_model = ModelStep(\n",
    "    name = \"PredMaintCreateModel\",\n",
    "    step_args = model.create(instance_type = \"ml.m5.large\"),\n",
    "    depends_on = [training_step.name]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Transform Step to Perform Batch Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sagemaker.transformer import Transformer\n",
    "\n",
    "transformer = Transformer(\n",
    "    model_name = step_create_model.properties.ModelName,\n",
    "    instance_type = \"ml.m5.xlarge\",\n",
    "    instance_count = 1,\n",
    "    output_path = f\"s3://{bucket}/{prefix}/PredMaintTransform\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TransformInput\n",
    "from sagemaker.workflow.steps import TransformStep\n",
    "\n",
    "step_transform = TransformStep(\n",
    "    name = \"PredMaintTransform\", transformer = transformer, inputs = TransformInput(data = batch_data)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Register Model Step to Create a Model Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "\n",
    "register_args = model.register(\n",
    "    content_types = [\"text/csv\"],\n",
    "    response_types = [\"text/csv\"],\n",
    "    inference_instances = [\"ml.t2.medium\", \"ml.m5.xlarge\"],\n",
    "    transform_instances = [\"ml.m5.xlarge\"],\n",
    "    model_package_group_name = \"PredMaintModelPackageGroupName\",\n",
    "    approval_status = model_approval_status,\n",
    ")\n",
    "step_register = ModelStep(name = \"PredMaintRegisterModel\", step_args = register_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Pipeline of Parameters, Steps, and Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "\n",
    "pipeline_name = \"PredMaintPipeline\"\n",
    "pipeline = Pipeline(\n",
    "    name = pipeline_name,\n",
    "    parameters = [\n",
    "        processing_instance_count,\n",
    "        instance_type,\n",
    "        model_approval_status,\n",
    "        input_data,\n",
    "        telemetry_data,\n",
    "        errors_data,\n",
    "        maint_data,\n",
    "        failures_data,\n",
    "        machines_data,\n",
    "        batch_data,\n",
    "        mse_threshold,\n",
    "    ],\n",
    "    steps = [preprocess_step, feature_store_step, train_test_split_step, training_step],\n",
    "#         steps = [step_process, step_train],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "definition = json.loads(pipeline.definition())\n",
    "definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn = sagemaker_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.lineage.visualizer import LineageTableVisualizer\n",
    "\n",
    "\n",
    "viz = LineageTableVisualizer(sagemaker.session.Session())\n",
    "for execution_step in reversed(execution.list_steps()):\n",
    "    print(execution_step)\n",
    "    display(viz.show(pipeline_execution_step=execution_step))\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Convert to datetime datatype\n",
    "def datetime_datatype(df):\n",
    "    print(\"Converting to type datetime\")\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    return df\n",
    "\n",
    "# Convert to category datatype\n",
    "def category_datatype(df, column_name):\n",
    "    print(\"Converting to type category\")\n",
    "    df[column_name] = df[column_name].astype('category')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Lag Features from Telemetry\n",
    "def telemetry_features(df):\n",
    "    df = datetime_datatype(df)\n",
    "    # Calculate mean values for telemetry features -- 3 hours rolling window\n",
    "    print(\"Calculate mean values for telemetry features -- 3 hours rolling window\")\n",
    "    temp = []\n",
    "    fields = ['volt', 'rotate', 'pressure', 'vibration']\n",
    "    for col in fields:\n",
    "        temp.append(pd.pivot_table(df,\n",
    "                                   index = 'datetime',\n",
    "                                   columns = 'machineID',\n",
    "                                   values = col).resample('3H', closed = 'left', label = 'right').mean().unstack())\n",
    "    telemetry_mean_3h = pd.concat(temp, axis = 1)\n",
    "    telemetry_mean_3h.columns = [i + 'mean_3h' for i in fields]\n",
    "    telemetry_mean_3h.reset_index(inplace = True)\n",
    "\n",
    "    # repeat for standard deviation\n",
    "    print(\"Calculate standard deviation for telemetry features -- 3 hours rolling window\")\n",
    "    temp = []\n",
    "    for col in fields:\n",
    "        temp.append(pd.pivot_table(df,\n",
    "                                   index = 'datetime',\n",
    "                                   columns = 'machineID',\n",
    "                                   values = col).resample('3H', closed = 'left', label = 'right').std().unstack())\n",
    "    telemetry_sd_3h = pd.concat(temp, axis = 1)\n",
    "    telemetry_sd_3h.columns = [i + 'sd_3h' for i in fields]\n",
    "    telemetry_sd_3h.reset_index(inplace = True)\n",
    "    \n",
    "    # Calculate mean values for telemetry features -- 24 hours rolling window\n",
    "    print(\"Calculate mean values for telemetry features -- 24 hours rolling window\")\n",
    "    temp = []\n",
    "    fields = ['volt', 'rotate', 'pressure', 'vibration']\n",
    "    for col in fields:\n",
    "        temp.append(pd.pivot_table(df,\n",
    "                                   index = 'datetime',\n",
    "                                   columns = 'machineID',\n",
    "                                   values = col)\n",
    "                    .resample('3H', closed = 'left', label = 'right')\n",
    "                    .first()\n",
    "                    .unstack()\n",
    "                    .rolling(window = 24, center = False).mean())\n",
    "    telemetry_mean_24h = pd.concat(temp, axis = 1)\n",
    "    telemetry_mean_24h.columns = [i + 'mean_24h' for i in fields]\n",
    "    telemetry_mean_24h.reset_index(inplace = True)\n",
    "    telemetry_mean_24h = telemetry_mean_24h.loc[-telemetry_mean_24h['voltmean_24h'].isnull()]\n",
    "\n",
    "    # repeat for standard deviation\n",
    "    print(\"Calculate standard deviation for telemetry features -- 24 hours rolling window\")\n",
    "    temp = []\n",
    "    fields = ['volt', 'rotate', 'pressure', 'vibration']\n",
    "    for col in fields:\n",
    "        temp.append(pd.pivot_table(df,\n",
    "                                   index = 'datetime',\n",
    "                                   columns = 'machineID',\n",
    "                                   values = col)\n",
    "                    .resample('3H', closed='left', label='right')\n",
    "                    .first()\n",
    "                    .unstack()\n",
    "                    .rolling(window = 24, center = False).std())\n",
    "    telemetry_sd_24h = pd.concat(temp, axis = 1)\n",
    "    telemetry_sd_24h.columns = [i + 'sd_24h' for i in fields]\n",
    "    telemetry_sd_24h = telemetry_sd_24h.loc[-telemetry_sd_24h['voltsd_24h'].isnull()]\n",
    "    telemetry_sd_24h.reset_index(inplace = True)\n",
    "    \n",
    "    telemetry_feat = pd.concat([telemetry_mean_3h,\n",
    "                            telemetry_sd_3h.iloc[:, 2:6],\n",
    "                            telemetry_mean_24h.iloc[:, 2:6],\n",
    "                            telemetry_sd_24h.iloc[:, 2:6]], axis = 1).dropna()\n",
    "    \n",
    "    return telemetry_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Lag Features for Errors\n",
    "def errors_lag_features(df):\n",
    "    df = datetime_datatype(df)\n",
    "    df = category_datatype(df, 'errorID')\n",
    "    print(\"Lag features for errors\")\n",
    "    error_count = pd.get_dummies(df.set_index('datetime')).reset_index()\n",
    "    error_count.columns = ['datetime', 'machineID', 'error1', 'error2', 'error3', 'error4', 'error5']\n",
    "    error_count = error_count.groupby(['machineID', 'datetime']).sum().reset_index()\n",
    "    error_count = telemetry[['datetime', 'machineID']].merge(error_count, on = ['machineID', 'datetime'], how = 'left').fillna(0.0)\n",
    "    temp = []\n",
    "    fields = ['error%d' % i for i in range(1, 6)]\n",
    "    for col in fields:\n",
    "        temp.append(pd.pivot_table(error_count,\n",
    "                                   index = 'datetime',\n",
    "                                   columns = 'machineID',\n",
    "                                   values = col)\n",
    "                    .resample('3H', closed='left', label='right')\n",
    "                    .first()\n",
    "                    .unstack()\n",
    "                    .rolling(window = 24, center = False).sum())\n",
    "    error_count = pd.concat(temp, axis = 1)\n",
    "    error_count.columns = [i + 'count' for i in fields]\n",
    "    error_count.reset_index(inplace = True)\n",
    "    error_count = error_count.dropna()\n",
    "    \n",
    "    return error_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Maintenance Features\n",
    "def maintenance_features(df):\n",
    "    df = datetime_datatype(df)\n",
    "    df = category_datatype(df, 'comp')\n",
    "    print(\"Maintenance Features -- Days since last replacement\")\n",
    "    comp_rep = pd.get_dummies(df.set_index('datetime')).reset_index()\n",
    "    comp_rep.columns = ['datetime', 'machineID', 'comp1', 'comp2', 'comp3', 'comp4']\n",
    "\n",
    "    # combine repairs for a given machine in a given hour\n",
    "    comp_rep = comp_rep.groupby(['machineID', 'datetime']).sum().reset_index()\n",
    "\n",
    "    # add timepoints where no components were replaced\n",
    "    comp_rep = telemetry[['datetime', 'machineID']].merge(comp_rep,\n",
    "                                                          on=['datetime', 'machineID'],\n",
    "                                                          how='outer').fillna(0).sort_values(by=['machineID', 'datetime'])\n",
    "    components = ['comp1', 'comp2', 'comp3', 'comp4']\n",
    "    for comp in components:\n",
    "        comp_rep.loc[comp_rep[comp] < 1, comp] = None\n",
    "        comp_rep.loc[-comp_rep[comp].isnull(),\n",
    "                     comp] = comp_rep.loc[-comp_rep[comp].isnull(), 'datetime']\n",
    "        comp_rep[comp] = comp_rep[comp].fillna(method = 'ffill')\n",
    "\n",
    "    comp_rep = comp_rep.loc[comp_rep['datetime'] > pd.to_datetime('2015-01-01')]\n",
    "    for comp in components:\n",
    "        comp_rep[comp] = (comp_rep[\"datetime\"] - pd.to_datetime(comp_rep[comp])) / np.timedelta64(1, \"D\")\n",
    "    \n",
    "    return comp_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Failures Features\n",
    "def failure_features(df):\n",
    "    print(\"Failure features\")\n",
    "    df = datetime_datatype(df)\n",
    "    df = category_datatype(df, 'failure')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Final Features\n",
    "def final_features(telemetry_df, errors_df, maint_df, machines_df):\n",
    "    print(\"Final features\")\n",
    "    final_feat = telemetry_df.merge(errors_df, on = ['datetime', 'machineID'], how = 'left')\n",
    "    final_feat = final_feat.merge(maint_df, on = ['datetime', 'machineID'], how = 'left')\n",
    "    final_feat = final_feat.merge(machines_df, on = ['machineID'], how = 'left')\n",
    "    return final_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to type datetime\n",
      "Calculate mean values for telemetry features -- 3 hours rolling window\n",
      "Calculate standard deviation for telemetry features -- 3 hours rolling window\n",
      "Calculate mean values for telemetry features -- 24 hours rolling window\n",
      "Calculate standard deviation for telemetry features -- 24 hours rolling window\n",
      "Converting to type datetime\n",
      "Converting to type category\n",
      "Lag features for errors\n",
      "Converting to type datetime\n",
      "Converting to type category\n",
      "Maintenance Features -- Days since last replacement\n",
      "Failure features\n",
      "Converting to type datetime\n",
      "Converting to type category\n",
      "Converting to type category\n"
     ]
    }
   ],
   "source": [
    "telemetry_df = telemetry_features(telemetry)\n",
    "errors_df = errors_lag_features(errors)\n",
    "maint_df = maintenance_features(maint)\n",
    "failures_df = failure_features(failures)\n",
    "machines_df = category_datatype(machines, 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Final Features -----\n",
      "Final features\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>machineID</th>\n",
       "      <th>datetime</th>\n",
       "      <th>voltmean_3h</th>\n",
       "      <th>rotatemean_3h</th>\n",
       "      <th>pressuremean_3h</th>\n",
       "      <th>vibrationmean_3h</th>\n",
       "      <th>voltsd_3h</th>\n",
       "      <th>rotatesd_3h</th>\n",
       "      <th>pressuresd_3h</th>\n",
       "      <th>vibrationsd_3h</th>\n",
       "      <th>...</th>\n",
       "      <th>error2count</th>\n",
       "      <th>error3count</th>\n",
       "      <th>error4count</th>\n",
       "      <th>error5count</th>\n",
       "      <th>comp1</th>\n",
       "      <th>comp2</th>\n",
       "      <th>comp3</th>\n",
       "      <th>comp4</th>\n",
       "      <th>model</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-04 06:00:00</td>\n",
       "      <td>186.092896</td>\n",
       "      <td>451.641253</td>\n",
       "      <td>107.989359</td>\n",
       "      <td>55.308074</td>\n",
       "      <td>13.489090</td>\n",
       "      <td>62.185045</td>\n",
       "      <td>5.118176</td>\n",
       "      <td>4.904365</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.000</td>\n",
       "      <td>217.000</td>\n",
       "      <td>157.000</td>\n",
       "      <td>172.000</td>\n",
       "      <td>model3</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-04 09:00:00</td>\n",
       "      <td>166.281848</td>\n",
       "      <td>453.787824</td>\n",
       "      <td>106.187582</td>\n",
       "      <td>51.990080</td>\n",
       "      <td>24.276228</td>\n",
       "      <td>23.621315</td>\n",
       "      <td>11.176731</td>\n",
       "      <td>3.394073</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.125</td>\n",
       "      <td>217.125</td>\n",
       "      <td>157.125</td>\n",
       "      <td>172.125</td>\n",
       "      <td>model3</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-04 12:00:00</td>\n",
       "      <td>175.412103</td>\n",
       "      <td>445.450581</td>\n",
       "      <td>100.887363</td>\n",
       "      <td>54.251534</td>\n",
       "      <td>34.918687</td>\n",
       "      <td>11.001625</td>\n",
       "      <td>10.580336</td>\n",
       "      <td>2.921501</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.250</td>\n",
       "      <td>217.250</td>\n",
       "      <td>157.250</td>\n",
       "      <td>172.250</td>\n",
       "      <td>model3</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-04 15:00:00</td>\n",
       "      <td>157.347716</td>\n",
       "      <td>451.882075</td>\n",
       "      <td>101.289380</td>\n",
       "      <td>48.602686</td>\n",
       "      <td>24.617739</td>\n",
       "      <td>28.950883</td>\n",
       "      <td>9.966729</td>\n",
       "      <td>2.356486</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.375</td>\n",
       "      <td>217.375</td>\n",
       "      <td>157.375</td>\n",
       "      <td>172.375</td>\n",
       "      <td>model3</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-04 18:00:00</td>\n",
       "      <td>176.450550</td>\n",
       "      <td>446.033068</td>\n",
       "      <td>84.521555</td>\n",
       "      <td>47.638836</td>\n",
       "      <td>8.071400</td>\n",
       "      <td>76.511343</td>\n",
       "      <td>2.636879</td>\n",
       "      <td>4.108621</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.500</td>\n",
       "      <td>217.500</td>\n",
       "      <td>157.500</td>\n",
       "      <td>172.500</td>\n",
       "      <td>model3</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   machineID            datetime  voltmean_3h  rotatemean_3h  pressuremean_3h  \\\n",
       "0          1 2015-01-04 06:00:00   186.092896     451.641253       107.989359   \n",
       "1          1 2015-01-04 09:00:00   166.281848     453.787824       106.187582   \n",
       "2          1 2015-01-04 12:00:00   175.412103     445.450581       100.887363   \n",
       "3          1 2015-01-04 15:00:00   157.347716     451.882075       101.289380   \n",
       "4          1 2015-01-04 18:00:00   176.450550     446.033068        84.521555   \n",
       "\n",
       "   vibrationmean_3h  voltsd_3h  rotatesd_3h  pressuresd_3h  vibrationsd_3h  \\\n",
       "0         55.308074  13.489090    62.185045       5.118176        4.904365   \n",
       "1         51.990080  24.276228    23.621315      11.176731        3.394073   \n",
       "2         54.251534  34.918687    11.001625      10.580336        2.921501   \n",
       "3         48.602686  24.617739    28.950883       9.966729        2.356486   \n",
       "4         47.638836   8.071400    76.511343       2.636879        4.108621   \n",
       "\n",
       "   ...  error2count  error3count  error4count  error5count   comp1    comp2  \\\n",
       "0  ...          0.0          0.0          0.0          0.0  22.000  217.000   \n",
       "1  ...          0.0          0.0          0.0          1.0  22.125  217.125   \n",
       "2  ...          0.0          0.0          0.0          1.0  22.250  217.250   \n",
       "3  ...          0.0          0.0          0.0          1.0  22.375  217.375   \n",
       "4  ...          0.0          0.0          0.0          1.0  22.500  217.500   \n",
       "\n",
       "     comp3    comp4   model  age  \n",
       "0  157.000  172.000  model3   18  \n",
       "1  157.125  172.125  model3   18  \n",
       "2  157.250  172.250  model3   18  \n",
       "3  157.375  172.375  model3   18  \n",
       "4  157.500  172.500  model3   18  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Label Construction -----\n",
      "----- Preprocessing completed -----\n"
     ]
    }
   ],
   "source": [
    "# Label Construction\n",
    "def label_construct(tele_df, error_df, maint_df, machine_df, failure_df):\n",
    "    print(\"----- Final Features -----\")\n",
    "    final_feat = final_features(tele_df, error_df, maint_df, machine_df)\n",
    "    display(final_feat.head())\n",
    "    \n",
    "    print(\"----- Label Construction -----\")\n",
    "    labeled_features = pd.DataFrame()\n",
    "    labeled_features = final_feat.merge(\n",
    "        failure_df, on = ['datetime', 'machineID'], how = 'left')\n",
    "    labeled_features['failure'] = labeled_features['failure'].astype(str)\n",
    "    labeled_features['failure'] = labeled_features['failure'].fillna(method = 'bfill', limit = 7)\n",
    "    labeled_features['failure'] = labeled_features['failure'].replace('nan', 'none')\n",
    "    print(\"----- Preprocessing completed -----\")\n",
    "    \n",
    "    return labeled_features\n",
    "\n",
    "labeled_features = label_construct(telemetry_df, errors_df, maint_df, machines_df, failures_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>machineID</th>\n",
       "      <th>datetime</th>\n",
       "      <th>voltmean_3h</th>\n",
       "      <th>rotatemean_3h</th>\n",
       "      <th>pressuremean_3h</th>\n",
       "      <th>vibrationmean_3h</th>\n",
       "      <th>voltsd_3h</th>\n",
       "      <th>rotatesd_3h</th>\n",
       "      <th>pressuresd_3h</th>\n",
       "      <th>vibrationsd_3h</th>\n",
       "      <th>...</th>\n",
       "      <th>error3count</th>\n",
       "      <th>error4count</th>\n",
       "      <th>error5count</th>\n",
       "      <th>comp1</th>\n",
       "      <th>comp2</th>\n",
       "      <th>comp3</th>\n",
       "      <th>comp4</th>\n",
       "      <th>model</th>\n",
       "      <th>age</th>\n",
       "      <th>failure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-04 06:00:00</td>\n",
       "      <td>186.092896</td>\n",
       "      <td>451.641253</td>\n",
       "      <td>107.989359</td>\n",
       "      <td>55.308074</td>\n",
       "      <td>13.489090</td>\n",
       "      <td>62.185045</td>\n",
       "      <td>5.118176</td>\n",
       "      <td>4.904365</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.000</td>\n",
       "      <td>217.000</td>\n",
       "      <td>157.000</td>\n",
       "      <td>172.000</td>\n",
       "      <td>model3</td>\n",
       "      <td>18</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-04 09:00:00</td>\n",
       "      <td>166.281848</td>\n",
       "      <td>453.787824</td>\n",
       "      <td>106.187582</td>\n",
       "      <td>51.990080</td>\n",
       "      <td>24.276228</td>\n",
       "      <td>23.621315</td>\n",
       "      <td>11.176731</td>\n",
       "      <td>3.394073</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.125</td>\n",
       "      <td>217.125</td>\n",
       "      <td>157.125</td>\n",
       "      <td>172.125</td>\n",
       "      <td>model3</td>\n",
       "      <td>18</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-04 12:00:00</td>\n",
       "      <td>175.412103</td>\n",
       "      <td>445.450581</td>\n",
       "      <td>100.887363</td>\n",
       "      <td>54.251534</td>\n",
       "      <td>34.918687</td>\n",
       "      <td>11.001625</td>\n",
       "      <td>10.580336</td>\n",
       "      <td>2.921501</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.250</td>\n",
       "      <td>217.250</td>\n",
       "      <td>157.250</td>\n",
       "      <td>172.250</td>\n",
       "      <td>model3</td>\n",
       "      <td>18</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-04 15:00:00</td>\n",
       "      <td>157.347716</td>\n",
       "      <td>451.882075</td>\n",
       "      <td>101.289380</td>\n",
       "      <td>48.602686</td>\n",
       "      <td>24.617739</td>\n",
       "      <td>28.950883</td>\n",
       "      <td>9.966729</td>\n",
       "      <td>2.356486</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.375</td>\n",
       "      <td>217.375</td>\n",
       "      <td>157.375</td>\n",
       "      <td>172.375</td>\n",
       "      <td>model3</td>\n",
       "      <td>18</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-04 18:00:00</td>\n",
       "      <td>176.450550</td>\n",
       "      <td>446.033068</td>\n",
       "      <td>84.521555</td>\n",
       "      <td>47.638836</td>\n",
       "      <td>8.071400</td>\n",
       "      <td>76.511343</td>\n",
       "      <td>2.636879</td>\n",
       "      <td>4.108621</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.500</td>\n",
       "      <td>217.500</td>\n",
       "      <td>157.500</td>\n",
       "      <td>172.500</td>\n",
       "      <td>model3</td>\n",
       "      <td>18</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   machineID            datetime  voltmean_3h  rotatemean_3h  pressuremean_3h  \\\n",
       "0          1 2015-01-04 06:00:00   186.092896     451.641253       107.989359   \n",
       "1          1 2015-01-04 09:00:00   166.281848     453.787824       106.187582   \n",
       "2          1 2015-01-04 12:00:00   175.412103     445.450581       100.887363   \n",
       "3          1 2015-01-04 15:00:00   157.347716     451.882075       101.289380   \n",
       "4          1 2015-01-04 18:00:00   176.450550     446.033068        84.521555   \n",
       "\n",
       "   vibrationmean_3h  voltsd_3h  rotatesd_3h  pressuresd_3h  vibrationsd_3h  \\\n",
       "0         55.308074  13.489090    62.185045       5.118176        4.904365   \n",
       "1         51.990080  24.276228    23.621315      11.176731        3.394073   \n",
       "2         54.251534  34.918687    11.001625      10.580336        2.921501   \n",
       "3         48.602686  24.617739    28.950883       9.966729        2.356486   \n",
       "4         47.638836   8.071400    76.511343       2.636879        4.108621   \n",
       "\n",
       "   ...  error3count  error4count  error5count   comp1    comp2    comp3  \\\n",
       "0  ...          0.0          0.0          0.0  22.000  217.000  157.000   \n",
       "1  ...          0.0          0.0          1.0  22.125  217.125  157.125   \n",
       "2  ...          0.0          0.0          1.0  22.250  217.250  157.250   \n",
       "3  ...          0.0          0.0          1.0  22.375  217.375  157.375   \n",
       "4  ...          0.0          0.0          1.0  22.500  217.500  157.500   \n",
       "\n",
       "     comp4   model  age  failure  \n",
       "0  172.000  model3   18     none  \n",
       "1  172.125  model3   18     none  \n",
       "2  172.250  model3   18     none  \n",
       "3  172.375  model3   18     none  \n",
       "4  172.500  model3   18     none  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "featurestore_runtime = boto_session.client(\n",
    "    service_name = \"sagemaker-featurestore-runtime\", region_name = region\n",
    ")\n",
    "\n",
    "feature_store_session = sagemaker.Session(\n",
    "    boto_session = boto_session,\n",
    "    sagemaker_client = sagemaker_boto_client,\n",
    "    sagemaker_featurestore_runtime_client = featurestore_runtime,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "telemetry_feature_group = FeatureGroup(name = 'telemetry_fg', sagemaker_session = sagemaker_session)\n",
    "errors_feature_group = FeatureGroup(name = 'errors_fg', sagemaker_session = sagemaker_session)\n",
    "maintenanace_feature_group = FeatureGroup(name = 'maintenance_fg', sagemaker_session = sagemaker_session)\n",
    "machines_feature_group = FeatureGroup(name = 'machines_fg', sagemaker_session = sagemaker_session)\n",
    "failures_feature_group = FeatureGroup(name = 'failures_fg', sagemaker_session = sagemaker_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### Feature Deifinitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "telemetry_definitions = {\n",
    "    'datetime': float,\n",
    "    'machineID': int,\n",
    "    'voltmean_3h': float,\n",
    "    'rotatemean_3h': float,\n",
    "    'pressuremean_3h': float,\n",
    "    'vibrationmean_3h': float,\n",
    "    'voltsd_3h': float,\n",
    "    'rotatesd_3h': float,\n",
    "    'pressuresd_3h': float,\n",
    "    'vibrationsd_3h': float,\n",
    "    'voltmean_24h': float,\n",
    "    'rotatemean_24h': float,\n",
    "    'pressuremean_24h': float,\n",
    "    'vibrationmean_24h': float,\n",
    "    'voltsd_24h': float,\n",
    "    'rotatesd_24h': float,\n",
    "    'pressuresd_24h': float,\n",
    "    'vibrationsd_24h': float\n",
    "}\n",
    "errors_definitions = {\n",
    "    \"machineID\": int,\n",
    "    \"datetime\": float,\n",
    "    \"error1count\": float,\n",
    "    \"error2count\": float,\n",
    "    \"error3count\": float,\n",
    "    \"error4count\": float,\n",
    "    \"error5count\": float\n",
    "}\n",
    "maint_definitions = {\n",
    "    \"machineID\": int,\n",
    "    \"datetime\": float,\n",
    "    \"comp1\": int,\n",
    "    \"comp2\": int,\n",
    "    \"comp3\": int,\n",
    "    \"comp4\": int\n",
    "}\n",
    "machines_definitions = {\n",
    "    \"machineID\": int,\n",
    "    \"datetime\": float,\n",
    "    \"model\": str,\n",
    "    \"age\": int\n",
    "}\n",
    "failure_definitions = {\n",
    "    \"machineID\": int,\n",
    "    \"datetime\": str,\n",
    "    \"failure\": str,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:451633145432:feature-group/failures_fg',\n",
       " 'ResponseMetadata': {'RequestId': '2bcfaaf4-d1c4-48d9-8933-1a21d96a22bb',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '2bcfaaf4-d1c4-48d9-8933-1a21d96a22bb',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '88',\n",
       "   'date': 'Fri, 21 Apr 2023 08:04:56 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failures_feature_group.load_feature_definitions(data_frame = fail)\n",
    "failures_feature_group.create(\n",
    "        s3_uri=f\"s3://{bucket}/{prefix}\",\n",
    "        record_identifier_name=\"machineID\",\n",
    "        event_time_feature_name=\"datetime\",\n",
    "        role_arn=sagemaker_role,\n",
    "        enable_online_store=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### Upload Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "s3_client.upload_file(\n",
    "    Filename = \"scripts/rf_script.py\", Bucket = bucket, Key = f\"{prefix}/scripts/rf_script.py\"\n",
    ")\n",
    "s3_client.upload_file(\n",
    "    Filename = \"scripts/train_test_split_script.py\", Bucket = bucket, Key = f\"{prefix}/scripts/train_test_split_script.py\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "rf_script_uri = f\"s3://{bucket}/{prefix}/scripts/rf_script.py\"\n",
    "train_test_script_uri = f\"s3://{bucket}/{prefix}/scripts/train_test_split_script.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### Train-Test Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from scripts.train_test_split_script import train_test_split_script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/Siddhant_MlOps/Preditive_Maintenance/scripts/train_test_split_script.py:11: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  'machineID',], 1))\n",
      "/root/Siddhant_MlOps/Preditive_Maintenance/scripts/train_test_split_script.py:13: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  'machineID'], 1))\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split_script(labeled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "with StringIO() as csv_buffer:\n",
    "    train_data.to_csv(csv_buffer, index = False)\n",
    "\n",
    "    response = s3_client.put_object(\n",
    "        Bucket = bucket, Key = f\"{prefix}/data/train-test/train.csv\", Body = csv_buffer.getvalue()\n",
    "    )\n",
    "\n",
    "    status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
    "\n",
    "    if status == 200:\n",
    "        print(f\"Successful S3 put_object response. Status - {status}\")\n",
    "    else:\n",
    "        print(f\"Unsuccessful S3 put_object response. Status - {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "with StringIO() as csv_buffer:\n",
    "    test_data.to_csv(csv_buffer, index = False)\n",
    "\n",
    "    response = s3_client.put_object(\n",
    "        Bucket = bucket, Key = f\"{prefix}/data/train-test/test.csv\", Body = csv_buffer.getvalue()\n",
    "    )\n",
    "\n",
    "    status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
    "\n",
    "    if status == 200:\n",
    "        print(f\"Successful S3 put_object response. Status - {status}\")\n",
    "    else:\n",
    "        print(f\"Unsuccessful S3 put_object response. Status - {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train_data_uri = f\"s3://{bucket}/{prefix}/data/train-test/train.csv\"\n",
    "test_data_uri = f\"s3://{bucket}/{prefix}/data/train-test/test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### SKLearn Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "FRAMEWORK_VERSION = \"1.0-1\"\n",
    "\n",
    "sklearn_estimator = SKLearn(\n",
    "    entry_point = \"scripts/rf_script.py\",\n",
    "    role = sagemaker_role,\n",
    "    instance_count = 1,\n",
    "    instance_type = \"ml.c5.xlarge\",\n",
    "    framework_version = FRAMEWORK_VERSION,\n",
    "    base_job_name = \"rf-scikit\",\n",
    "    output_path = f\"s3://{bucket}/{prefix}/estimatoe-output/rf-scikit\",\n",
    "    hyperparameters = {\n",
    "        \"n-estimators\": 100,\n",
    "        \"min-samples-leaf\": 3,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "sklearn_estimator.fit(inputs = {\"train\": train_data_uri, \"test\": test_data_uri}, wait = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'wait'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-3402e6967bb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msklearn_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m artifact = sagemaker_boto_client.describe_training_job(\n\u001b[1;32m      3\u001b[0m     \u001b[0mTrainingJobName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'wait'"
     ]
    }
   ],
   "source": [
    "sklearn_estimator.latest_training_job.wait(logs = \"None\")\n",
    "artifact = sagemaker_boto_client.describe_training_job(\n",
    "    TrainingJobName = sklearn_estimator.latest_training_job.name\n",
    ")[\"ModelArtifacts\"][\"S3ModelArtifacts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['voltmean_3h', 'rotatemean_3h', 'pressuremean_3h', 'vibrationmean_3h',\n",
       "       'voltsd_3h', 'rotatesd_3h', 'pressuresd_3h', 'vibrationsd_3h',\n",
       "       'voltmean_24h', 'rotatemean_24h', 'pressuremean_24h',\n",
       "       'vibrationmean_24h', 'voltsd_24h', 'rotatesd_24h', 'pressuresd_24h',\n",
       "       'vibrationsd_24h', 'error1count', 'error2count', 'error3count',\n",
       "       'error4count', 'error5count', 'comp1', 'comp2', 'comp3', 'comp4', 'age',\n",
       "       'model_model1', 'model_model2', 'model_model3', 'model_model4',\n",
       "       'failure_comp1', 'failure_comp2', 'failure_comp3', 'failure_comp4',\n",
       "       'failure_none'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.columns"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
